{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"getting_started.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPukkeBdhAwioZzeJsFGy8n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jwi2y70ermV0"},"source":["## Dataset:\n"]},{"cell_type":"markdown","metadata":{"id":"IJfyzO6ABrNz"},"source":["### Deep Noise Suppression Challenge - INTERSPEECH 2020 (DNS-INTERSPEECH-2020)\n","It consists of 65000 speech clips (the 30s per clip) and 65000 noise clips (10s per clip). You can download this dataset from https://github.com/microsoft/DNS-Challenge.git. This Git repository contains the DNS Challenge dataset (INTERSPEECH 2020) and the newer DNS Challenge dataset (ICASSP 2021). The default branch of the Git repository is the ICASSP 2021 Dataset. You need to check out the default branch to the interspeech2020 branch."]},{"cell_type":"markdown","metadata":{"id":"AfYji2DnADmZ"},"source":["## Usage"]},{"cell_type":"markdown","metadata":{"id":"Cl_XQSCeCFCB"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"PO8XQZt_AXd-"},"source":["First, we need to enter a directory named after the dataset, such as `dns_interspeech_2020`. Then, we can call the default training configuration:\n","\n","```shell\n","cd FullSubNet/recipes/dns_interspeech_2020\n","\n","# Use a default config and two GPUs to train the FullSubNet model\n","CUDA_VISIABLE_DEVICES=0,1 python train.py -C fullsubnet/train.toml -N 2\n","\n","# Use default config and one GPU to train the Fullband baseline model\n","CUDA_VISIABLE_DEVICES=0 python train.py -C fullband_baseline/train.toml -N 1\n","\n","# Resume the experiment using \"-R\" parameter\n","CUDA_VISIABLE_DEVICES=0,1 python train.py -C fullband_baseline/train.toml -N 2 -R\n","```\n","\n","See more details in `FullSubNet/recipes/dns_interspeech_2020/train.py` and `FullSubNet/recipes/dns_interspeech_2020/**/train.toml`."]},{"cell_type":"markdown","metadata":{"id":"BAuUXGemAyyE"},"source":["### Logs and Visualization"]},{"cell_type":"markdown","metadata":{"id":"arB3BqPBA4Zj"},"source":["The logs during the training will be stored, and we can visualize it using TensorBoard. Assuming that:\n","\n","- The file path of the training configuration is `FullSubNet/recipes/dns_interspeech_2020/fullsubnet/train.toml`\n","- In the training configuration, the key `save_dir` is `\"~/Experiments/FullSubNet\"`\n","\n","Then the log information will be stored in the `~/Experiments/FullSubNet/train` directory. This directory contains the following:\n","\n","- `logs/` directory: store the TensorBoard related data, including loss curves, audio files, and spectrogram figures.\n","- `checkpoints/` directory: stores all checkpoints during training, from which you can resume the training or start an inference.\n","- `*.toml` file: the backup of the current training configuration.\n","\n"," In the `logs/` directory, use the following command to visualize loss curves, spectrogram figures, and audio files during the training and the validation.\n","\n","```shell\n","tensorboard --logdir ~/Experiments/FullSubNet/train\n","\n","# specify a port 45454\n","tensorboard --logdir ~/Experiments/FullSubNet/train --port 45454\n","```"]},{"cell_type":"markdown","metadata":{"id":"xabqlvSdA-Fi"},"source":["### Inference"]},{"cell_type":"markdown","metadata":{"id":"qvfNnJFxBDXj"},"source":["After training, you can enhance noisy speech. Take the FullSubNet as an example:\n","\n","1. Checking the noisy speech directory path and the sample rate in `FullSubNet/recipes/dns_interspeech_2020/fullsubnet/inference.toml`.\n","\n","```toml\n","[dataset.args]\n","dataset_dir_list = [\n","    \"/path/to/your/dataset_1/\",\n","    \"/path/to/your/dataset_2/\",\n","    \"others...\"\n","]\n","sr = 16000\n","```\n","\n","2. Switch to `FullSubNet/recipes/dns_interspeech_2020` directory and start inference:\n","\n","```shell\n","cd FullSubNet/recipes/dns_interspeech_2020\n","\n","# One GPU is used by default\n","python inference.py \\\n","  -C fullsubnet/inference.toml \\\n","  -M /path/to/your/checkpoint_dir/best_model.tar \\\n","  -O /path/to/your/enhancement/dir\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"AnPM0gyeBIix"},"source":["### Applying a Pre-trained Model"]},{"cell_type":"markdown","metadata":{"id":"zs09WCB9BMOp"},"source":["Or, in the inference stage, you can use a pre-trained model downloaded from the [Releases Page](https://github.com/haoxiangsnr/FullSubNet/releases):\n","\n","1. As mentioned above, you need to check the noisy speech directory path and the sample rate in `FullSubNet/recipes/dns_interspeech_2020/fullsubnet/inference.toml` are correct.\n","2. Change the \"-M\" parameter to the path of the pre-trained model downloaded from the Releases Page.\n","\n","Check more details of inference parameters in `FullSubNet/recipes/dns_interspeech_2020/fullsubnet/inference.toml`.\n"]},{"cell_type":"markdown","metadata":{"id":"WThfAAX1BTgJ"},"source":["### Metrics"]},{"cell_type":"markdown","metadata":{"id":"DZrcYz11BXov"},"source":["Calculating metrics (SI_SDR, STOI, WB_PESQ, NB_PESQ, etc.) using the following command lines:\n","\n","```shell\n","# Switching path\n","cd FullSubNet\n","\n","# DNS-INTERSPEECH-2020\n","python tools/calculate_metrics.py \\\n","  -R /path/to/reference/ \\\n","  -E /path/to/enhancement/ \\\n","  -M SI_SDR,STOI,WB_PESQ,NB_PESQ \\\n","  -S DNS_1\n","```"]},{"cell_type":"code","metadata":{"id":"XYFYOgqvAVKs"},"source":[""],"execution_count":null,"outputs":[]}]}
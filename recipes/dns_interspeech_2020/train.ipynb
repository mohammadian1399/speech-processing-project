{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtOnOOuO9BcW","executionInfo":{"status":"ok","timestamp":1628370608444,"user_tz":-270,"elapsed":391,"user":{"displayName":"Erfan vahabi amlashi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLmwTLAVre_U4OjYmvcJ_QFCfpM97XNS2GEfbuLw=s64","userId":"15723382876419201026"}},"outputId":"5b9e097e-a18d-494a-f6b1-9a96dda9b67f"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","% cd '/content/gdrive/MyDrive/Colab Notebooks/FullSubNet/recipes/dns_interspeech_2020/'\n","! ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/Colab Notebooks/FullSubNet/recipes/dns_interspeech_2020\n","dataset_inference.py   fullband_baseline  inferencer.ipynb\n","dataset_train.py       fullsubnet\t  __init__.py\n","dataset_validation.py  inference.ipynb\t  train.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qKZv63IdOC1s","executionInfo":{"status":"ok","timestamp":1628370610538,"user_tz":-270,"elapsed":525,"user":{"displayName":"Erfan vahabi amlashi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLmwTLAVre_U4OjYmvcJ_QFCfpM97XNS2GEfbuLw=s64","userId":"15723382876419201026"}}},"source":["import argparse\n","import os\n","import random\n","import socket\n","import sys\n","from pathlib import Path\n","\n","import numpy as np\n","import toml\n","import torch\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","from torch.utils.data import DataLoader, DistributedSampler"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPmCcRE7OJwO","executionInfo":{"status":"ok","timestamp":1628370610539,"user_tz":-270,"elapsed":5,"user":{"displayName":"Erfan vahabi amlashi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLmwTLAVre_U4OjYmvcJ_QFCfpM97XNS2GEfbuLw=s64","userId":"15723382876419201026"}}},"source":["sys.path.append(os.path.abspath('../../.'))  # add /path/to/FullSubNet\n","import audio_zen.loss as loss\n","from audio_zen.utils import initialize_module"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9y5sBEwBEko","executionInfo":{"status":"ok","timestamp":1628370611028,"user_tz":-270,"elapsed":7,"user":{"displayName":"Erfan vahabi amlashi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLmwTLAVre_U4OjYmvcJ_QFCfpM97XNS2GEfbuLw=s64","userId":"15723382876419201026"}}},"source":["def entry(rank, world_size, free_port, config, resume, only_validation):\n","    torch.manual_seed(config[\"meta\"][\"seed\"])  # For both CPU and GPU\n","    np.random.seed(config[\"meta\"][\"seed\"])\n","    random.seed(config[\"meta\"][\"seed\"])\n","\n","    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n","    os.environ[\"MASTER_PORT\"] = str(free_port)  # A random local port\n","\n","    # Initialize the process group\n","    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n","    print(f\"{rank + 1}/{world_size} process initialized.\")\n","\n","    # The DistributedSampler will split the dataset into the several cross-process parts.\n","    # On the contrary, \"Sampler=None, shuffle=True\", each GPU will get all data in the whole dataset.\n","    train_dataset = initialize_module(config[\"train_dataset\"][\"path\"], args=config[\"train_dataset\"][\"args\"])\n","    sampler = DistributedSampler(dataset=train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n","    train_dataloader = DataLoader(\n","        dataset=train_dataset,\n","        sampler=sampler,\n","        shuffle=False,\n","        **config[\"train_dataset\"][\"dataloader\"],\n","    )\n","\n","    valid_dataloader = DataLoader(\n","        dataset=initialize_module(config[\"validation_dataset\"][\"path\"], args=config[\"validation_dataset\"][\"args\"]),\n","        num_workers=0,\n","        batch_size=1\n","    )\n","\n","    model = initialize_module(config[\"model\"][\"path\"], args=config[\"model\"][\"args\"])\n","\n","    optimizer = torch.optim.Adam(\n","        params=model.parameters(),\n","        lr=config[\"optimizer\"][\"lr\"],\n","        betas=(config[\"optimizer\"][\"beta1\"], config[\"optimizer\"][\"beta2\"])\n","    )\n","\n","    loss_function = getattr(loss, config[\"loss_function\"][\"name\"])(**config[\"loss_function\"][\"args\"])\n","\n","    trainer_class = initialize_module(config[\"trainer\"][\"path\"], initialize=False)\n","\n","    trainer = trainer_class(\n","        dist=dist,\n","        rank=rank,\n","        config=config,\n","        resume=resume,\n","        only_validation=only_validation,\n","        model=model,\n","        loss_function=loss_function,\n","        optimizer=optimizer,\n","        train_dataloader=train_dataloader,\n","        validation_dataloader=valid_dataloader\n","    )\n","\n","    trainer.train()\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"id":"-LU0fOFxBQaM","executionInfo":{"status":"error","timestamp":1628373734102,"user_tz":-270,"elapsed":998,"user":{"displayName":"Erfan vahabi amlashi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLmwTLAVre_U4OjYmvcJ_QFCfpM97XNS2GEfbuLw=s64","userId":"15723382876419201026"}},"outputId":"f87850e8-028d-4c7d-be48-375cfd126bde"},"source":["# parser = argparse.ArgumentParser(description=\"FullSubNet\")\n","# parser.add_argument(\"-C\", \"--configuration\", required=True, type=str, help=\"Configuration (*.toml).\")\n","# parser.add_argument(\"-R\", \"--resume\", action=\"store_true\", help=\"Resume the experiment from latest checkpoint.\")\n","# parser.add_argument(\"-V\", \"--only_validation\", action=\"store_true\", help=\"Only run validation. It is used for debugging validation.\")\n","# parser.add_argument(\"-N\", \"--num_gpus\", type=int, default=2, help=\"The number of GPUs you are using for training.\")\n","# parser.add_argument(\"-P\", \"--preloaded_model_path\", type=str, help=\"Path of the *.pth file of a model.\")\n","class DotDict(dict):\n","    pass\n","args = DotDict()\n","args.configuration = \"fullsubnet/train.toml\"\n","args.resume = False\n","args.only_validation = False\n","args.num_gpus = 1\n","args.preloaded_model_path = \"\"\n","\n","if args.preloaded_model_path:\n","    assert not args.resume, \"The 'resume' conflicts with the 'preloaded_model_path'.\"\n","\n","config_path = Path(args.configuration).expanduser().absolute()\n","configuration = toml.load(config_path.as_posix())\n","\n","# append the parent dir of the config path to python's context\n","# /path/to/recipes/dns_interspeech_2020/exp/'\n","sys.path.append(config_path.parent.as_posix())\n","\n","configuration[\"meta\"][\"experiment_name\"], _ = os.path.splitext(os.path.basename(args.configuration))\n","configuration[\"meta\"][\"config_path\"] = args.configuration\n","configuration[\"meta\"][\"preloaded_model_path\"] = args.preloaded_model_path\n","\n","socket_stream = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","socket_stream.bind((\"\", 0))\n","socket_stream.listen(1)\n","free_port = socket_stream.getsockname()[1]\n","socket_stream.close()\n","\n","# Expand python search path to \"recipes\"\n","# sys.path.append(os.path.join(os.getcwd(), \"..\"))\n","\n","# One training job is corresponding to one group (world).\n","# The world size is the number of processes for training, which is usually the number of GPUs you are using for distributed training.\n","# the rank is the unique ID given to a process.\n","# Find more information about DistributedDataParallel (DDP) in https://pytorch.org/tutorials/intermediate/ddp_tutorial.html.\n","entry(0, args.num_gpus, free_port, configuration, args.resume, args.only_validation)\n","# mp.spawn(entry,\n","#           args=(args.num_gpus, free_port, configuration, args.resume, args.only_validation),\n","#           nprocs=args.num_gpus,\n","#           join=True)\n"],"execution_count":7,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-614ff9e64902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# the rank is the unique ID given to a process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Find more information about DistributedDataParallel (DDP) in https://pytorch.org/tutorials/intermediate/ddp_tutorial.html.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfree_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m# mp.spawn(entry,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#           args=(args.num_gpus, free_port, configuration, args.resume, args.only_validation),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-4444c36cbe95>\u001b[0m in \u001b[0;36mentry\u001b[0;34m(rank, world_size, free_port, config, resume, only_validation)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Initialize the process group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nccl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{rank + 1}/{world_size} process initialized.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mGroupMember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORLD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         raise RuntimeError(\"trying to initialize the default process group \"\n\u001b[0m\u001b[1;32m    485\u001b[0m                            \"twice!\")\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: trying to initialize the default process group twice!"]}]},{"cell_type":"code","metadata":{"id":"WdAOq3a0U6qH"},"source":[""],"execution_count":null,"outputs":[]}]}